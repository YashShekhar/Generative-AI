{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YashShekhar/Generative-AI/blob/main/Build_a_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20985327",
      "metadata": {
        "id": "20985327"
      },
      "source": [
        "# Build a Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "spsA2yn9FVRf",
        "outputId": "e7fcb3f8-1a6b-4cd5-9987-88a2a04a5318"
      },
      "id": "spsA2yn9FVRf",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.3.74)\n",
            "Requirement already satisfied: groq<1,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.31.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (4.14.1)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (25.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.30.0->langchain-groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-core langgraph>0.2.27"
      ],
      "metadata": {
        "id": "Yw_odV4JGUWE"
      },
      "id": "Yw_odV4JGUWE",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dd14970e",
      "metadata": {
        "id": "dd14970e"
      },
      "outputs": [],
      "source": [
        "## Setup API KEY - Google Colab\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9f38d231",
      "metadata": {
        "id": "9f38d231"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    model_provider=\"groq\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bbe76d0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbe76d0c",
        "outputId": "e24b8464-4a76-4b9e-d11d-820ba4f915ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello Yash! 👋 Nice to meet you. How can I help you today?', additional_kwargs={'reasoning_content': 'The user says \"Hi! I am Yash\". Likely a greeting, maybe wants to respond. Should respond friendly. No special instructions.'}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 77, 'total_tokens': 134, 'completion_time': 0.115624988, 'prompt_time': 0.003754858, 'queue_time': 0.216534622, 'total_time': 0.119379846}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_213abb2467', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--6996cecf-f420-49f4-a955-e7d6a6a71c3f-0', usage_metadata={'input_tokens': 77, 'output_tokens': 57, 'total_tokens': 134})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "model.invoke([HumanMessage(content=\"Hi! I am Yash\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4068ae0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4068ae0a",
        "outputId": "913b2e8a-2c81-4e54-e8fd-fa85f130c979"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I’m not able to see your name. If you’d like me to address you personally, just let me know what you’d prefer to be called!', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" The assistant doesn\\'t have any prior info about the user. According to policy, we must respond that we don\\'t know. We can ask politely for clarification.'}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 75, 'total_tokens': 154, 'completion_time': 0.156947468, 'prompt_time': 0.003092402, 'queue_time': 0.212843506, 'total_time': 0.16003987}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8db49de948', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5f2c8406-fef1-4eaf-be11-272d05c5fc10-0', usage_metadata={'input_tokens': 75, 'output_tokens': 79, 'total_tokens': 154})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model.invoke([HumanMessage(content=\"What's my name?\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "8835fa42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8835fa42",
        "outputId": "403b9b17-5eb1-42a0-963c-80c16399d548"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Yash.', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" They previously said \"Hi! I am Yash\". So answer: Yash. Should respond accordingly.'}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 104, 'total_tokens': 149, 'completion_time': 0.090939996, 'prompt_time': 0.005472156, 'queue_time': 0.568958771, 'total_time': 0.096412152}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_a09bde29de', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2e138b43-c537-44a0-bbae-50c3b91aaa13-0', usage_metadata={'input_tokens': 104, 'output_tokens': 45, 'total_tokens': 149})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"Hi! I am Yash\"),\n",
        "        AIMessage(content='Hello Yash! 👋 How can I help you today?'),\n",
        "        HumanMessage(content=\"What's my name?\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Message Persistence"
      ],
      "metadata": {
        "id": "R0CwXaZ2GA0O"
      },
      "id": "R0CwXaZ2GA0O"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6d4a4875",
      "metadata": {
        "id": "6d4a4875"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "  response = model.invoke(state[\"messages\"])\n",
        "  return {\"messages\": response}\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "DNxqxM3eGOld"
      },
      "id": "DNxqxM3eGOld",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
      ],
      "metadata": {
        "id": "kViOe8l0HGC-"
      },
      "id": "kViOe8l0HGC-",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Hi! I'm Yash.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l5FGPpWHjAb",
        "outputId": "2fc72cf3-239b-465f-af84-1650800695ae"
      },
      "id": "5l5FGPpWHjAb",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='2ef7dfe9-8a1c-4f77-86f6-ba1dc62078b6'),\n",
              "  AIMessage(content='I don’t have any information about your name. If you’d like, feel free to tell me what you’d like to be called, and I’ll use that!', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" There\\'s no prior context. According to policy, we must not fabricate personal info. We should respond that we don\\'t have that info. Apologize and ask if they\\'d like to share.'}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 75, 'total_tokens': 164, 'completion_time': 0.188806465, 'prompt_time': 0.003646782, 'queue_time': 0.53532412, 'total_time': 0.192453247}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8db49de948', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--27406027-b41e-482e-abd2-0e4015066401-0', usage_metadata={'input_tokens': 75, 'output_tokens': 89, 'total_tokens': 164}),\n",
              "  HumanMessage(content=\"Hi! I'm Yash.\", additional_kwargs={}, response_metadata={}, id='6dfbdf3c-e628-48b4-a976-b1d6c1935be2'),\n",
              "  AIMessage(content='Nice to meet you, Yash! How can I help you today?', additional_kwargs={'reasoning_content': \"The user says Hi! I'm Yash. Should respond friendly, address them as Yash. No disallowed content.\"}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 125, 'total_tokens': 174, 'completion_time': 0.104868901, 'prompt_time': 0.005078415, 'queue_time': 0.27024528, 'total_time': 0.109947316}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_213abb2467', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--46445912-9b9d-4231-a0b8-34320791d2d0-0', usage_metadata={'input_tokens': 125, 'output_tokens': 49, 'total_tokens': 174})]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJT2V-wnHs_R",
        "outputId": "6c370043-de1d-429a-d569-69747e3e1f41"
      },
      "id": "dJT2V-wnHs_R",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Nice to meet you, Yash! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdidJYldIN1z",
        "outputId": "c7858db4-30c8-4ffa-b66c-da8bfa78089b"
      },
      "id": "KdidJYldIN1z",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Yash.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we change the config i.e., thread_id. It will start a new conversation\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWd1RwHRITfR",
        "outputId": "ca2103aa-f991-4ea7-ffc8-c845065d44f8"
      },
      "id": "lWd1RwHRITfR",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I don’t have any information about your name. If you’d like to share it, feel free to let me know!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wWlNTXCIf5T",
        "outputId": "f9a5566b-674e-4c1f-a2f2-23bac990d80a"
      },
      "id": "9wWlNTXCIf5T",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Yash.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
      ],
      "metadata": {
        "id": "2allI_lKIjyr"
      },
      "id": "2allI_lKIjyr",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You talk like a pirate. Answer all questions to the best of your ability.\"\n",
        "    ),\n",
        "    MessagesPlaceholder(variable_name=\"messages\")\n",
        "])\n",
        "prompt_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NgypP34KHm_",
        "outputId": "c7e17343-a8b4-4506-e99d-fb9b03c28430"
      },
      "id": "3NgypP34KHm_",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7e72d2357d80>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You talk like a pirate. Answer all questions to the best of your ability.'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages')])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph"
      ],
      "metadata": {
        "id": "vdmi8stXKiNB"
      },
      "id": "vdmi8stXKiNB",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "  prompt = prompt_template.invoke(state)\n",
        "  response = model.invoke(prompt)\n",
        "  return {\"messages\": response}\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "2FRhD1O6Kr1O"
      },
      "id": "2FRhD1O6Kr1O",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc345\"}}"
      ],
      "metadata": {
        "id": "cmQMljN1K1rQ"
      },
      "id": "cmQMljN1K1rQ",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Hi! I'm Jim.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80y6HdyfK3Ss",
        "outputId": "30ca9c90-1f2c-40ec-ff7d-8839bc87f594"
      },
      "id": "80y6HdyfK3Ss",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Ahoy, Jim! 'Tis a fine day on the digital seas, be it? How can this salty seadog be of service t' ye? Arr!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ajC6rbMK6Es",
        "outputId": "f1e9dfc5-ffe7-4e81-e71d-641499eeb705"
      },
      "id": "6ajC6rbMK6Es",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Yer name be Jim, matey! Arr!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "legkO_MqK_Je"
      },
      "id": "legkO_MqK_Je",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "class State(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "  language: str\n",
        "\n",
        "workflow = StateGraph(state_schema=State)\n",
        "\n",
        "def call_model(state: State):\n",
        "  prompt = prompt_template.invoke(state)\n",
        "  response = model.invoke(prompt)\n",
        "  return {\"messages\": [response]}\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "momory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "trSoDCTFLIC-"
      },
      "id": "trSoDCTFLIC-",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
        "query = \"Hi! I'm Bob.\"\n",
        "language = \"Spanish\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lIoGA-ALpLC",
        "outputId": "72723dd1-234e-4548-a7ea-9cb364b56870"
      },
      "id": "-lIoGA-ALpLC",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "¡Hola, Bob! ¿En qué puedo ayudarte hoy?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5TLvJYGNBzb",
        "outputId": "e6fb097f-2926-4e8a-cd2b-abd962e2c74b"
      },
      "id": "R5TLvJYGNBzb",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Tu nombre es Bob.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, trim_messages"
      ],
      "metadata": {
        "id": "hSgQlc1XNlfc"
      },
      "id": "hSgQlc1XNlfc",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trimmer = trim_messages(\n",
        "    max_tokens=65,\n",
        "    strategy=\"last\",\n",
        "    token_counter=model,\n",
        "    include_system=True,\n",
        "    allow_partial=False,\n",
        "    start_on=\"human\",\n",
        ")\n",
        "trimmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k6H_TBQNl_M",
        "outputId": "5e13857b-ed23-4472-8dc3-0575ca029b17"
      },
      "id": "5k6H_TBQNl_M",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableLambda(...)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"you're a good assistant\"),\n",
        "    HumanMessage(content=\"hi! I'm bob\"),\n",
        "    AIMessage(content=\"hi!\"),\n",
        "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
        "    AIMessage(content=\"nice\"),\n",
        "    HumanMessage(content=\"whats 2 + 2\"),\n",
        "    AIMessage(content=\"4\"),\n",
        "    HumanMessage(content=\"thanks\"),\n",
        "    AIMessage(content=\"no problem!\"),\n",
        "    HumanMessage(content=\"having fun?\"),\n",
        "    AIMessage(content=\"yes!\"),\n",
        "]"
      ],
      "metadata": {
        "id": "vnuFJ8rTNv9Z"
      },
      "id": "vnuFJ8rTNv9Z",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trimmer.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UDCGDN3N0Jv",
        "outputId": "2c0e0a1d-27a7-48af-93c8-3ac07b7a8a2a"
      },
      "id": "_UDCGDN3N0Jv",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(state_schema=State)\n",
        "\n",
        "def call_model(state: State):\n",
        "  print(f\"Messages before trimming: {len(state['messages'])}\")\n",
        "  trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
        "  print(f\"Messages after trimming: {len(trimmed_messages)}\")\n",
        "  print(\"Remaining messages:\")\n",
        "  for msg in trimmed_messages:\n",
        "        print(f\"  {type(msg).__name__}: {msg.content}\")\n",
        "  prompt = prompt_template.invoke(\n",
        "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
        "    )\n",
        "  response = model.invoke(prompt)\n",
        "  return {\"messages\": [response]}\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "momory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n"
      ],
      "metadata": {
        "id": "9qgMSiwVN6tQ"
      },
      "id": "9qgMSiwVN6tQ",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
        "query = \"What is my name?\"\n",
        "language = \"English\"\n",
        "\n",
        "input_messages = messages + [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Mbm3eZVOxNb",
        "outputId": "5acd5495-92bb-41f5-d71c-841b582205d4"
      },
      "id": "5Mbm3eZVOxNb",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Messages before trimming: 14\n",
            "Messages after trimming: 12\n",
            "Remaining messages:\n",
            "  SystemMessage: you're a good assistant\n",
            "  HumanMessage: I like vanilla ice cream\n",
            "  AIMessage: nice\n",
            "  HumanMessage: whats 2 + 2\n",
            "  AIMessage: 4\n",
            "  HumanMessage: thanks\n",
            "  AIMessage: no problem!\n",
            "  HumanMessage: having fun?\n",
            "  AIMessage: yes!\n",
            "  HumanMessage: What is my name?\n",
            "  AIMessage: Your name is Bob.\n",
            "  HumanMessage: What is my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I don’t actually know your name. If you’d like to share it, feel free to let me know!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
        "\n",
        "query = \"What math problem was asked?\"\n",
        "language = \"English\"\n",
        "\n",
        "input_messages = messages + [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIlhK28nO0Nd",
        "outputId": "c6c63d1a-1e7b-4636-b509-c5968092c381"
      },
      "id": "bIlhK28nO0Nd",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Messages before trimming: 12\n",
            "Messages after trimming: 12\n",
            "Remaining messages:\n",
            "  SystemMessage: you're a good assistant\n",
            "  HumanMessage: hi! I'm bob\n",
            "  AIMessage: hi!\n",
            "  HumanMessage: I like vanilla ice cream\n",
            "  AIMessage: nice\n",
            "  HumanMessage: whats 2 + 2\n",
            "  AIMessage: 4\n",
            "  HumanMessage: thanks\n",
            "  AIMessage: no problem!\n",
            "  HumanMessage: having fun?\n",
            "  AIMessage: yes!\n",
            "  HumanMessage: What math problem was asked?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The math problem you asked earlier was **“What is 2 + 2?”**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming"
      ],
      "metadata": {
        "id": "53GDph7BPTbP"
      },
      "id": "53GDph7BPTbP"
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
        "query = \"Hi I'm Todd, please tell me a joke.\"\n",
        "language = \"English\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "for chunk, metadata in app.stream(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        "    stream_mode=\"messages\",\n",
        "):\n",
        "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
        "        print(chunk.content, end=\"|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anHmu8ZEPESF",
        "outputId": "0fe841c5-0364-4140-eeb7-eadb7edf55c5"
      },
      "id": "anHmu8ZEPESF",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Messages before trimming: 7\n",
            "Messages after trimming: 1\n",
            "Remaining messages:\n",
            "  HumanMessage: Hi I'm Todd, please tell me a joke.\n",
            "||||||||||||||||||Sure| thing|,| Todd|!| Here|’s| one| for| you|:\n",
            "\n",
            "|**|Why| don|’t| scientists| trust| atoms| anymore|?|**\n",
            "\n",
            "|Because| they| *|make| up|*| everything|!| 😄||"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2yM0k6ftPa9V"
      },
      "id": "2yM0k6ftPa9V",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}