{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YashShekhar/Generative-AI/blob/main/Build_a_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20985327",
      "metadata": {
        "id": "20985327"
      },
      "source": [
        "# Build a Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "spsA2yn9FVRf",
        "outputId": "8da90657-a9dc-44b1-a25a-3b745b5d556f"
      },
      "id": "spsA2yn9FVRf",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.3.74)\n",
            "Requirement already satisfied: groq<1,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.31.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain-groq) (4.14.1)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (25.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.30.0->langchain-groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-core langgraph>0.2.27"
      ],
      "metadata": {
        "id": "Yw_odV4JGUWE"
      },
      "id": "Yw_odV4JGUWE",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dd14970e",
      "metadata": {
        "id": "dd14970e"
      },
      "outputs": [],
      "source": [
        "## Setup API KEY - Google Colab\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9f38d231",
      "metadata": {
        "id": "9f38d231"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    model_provider=\"groq\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bbe76d0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbe76d0c",
        "outputId": "e24b8464-4a76-4b9e-d11d-820ba4f915ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello Yash! ðŸ‘‹ Nice to meet you. How can I help you today?', additional_kwargs={'reasoning_content': 'The user says \"Hi! I am Yash\". Likely a greeting, maybe wants to respond. Should respond friendly. No special instructions.'}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 77, 'total_tokens': 134, 'completion_time': 0.115624988, 'prompt_time': 0.003754858, 'queue_time': 0.216534622, 'total_time': 0.119379846}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_213abb2467', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--6996cecf-f420-49f4-a955-e7d6a6a71c3f-0', usage_metadata={'input_tokens': 77, 'output_tokens': 57, 'total_tokens': 134})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "model.invoke([HumanMessage(content=\"Hi! I am Yash\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4068ae0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4068ae0a",
        "outputId": "913b2e8a-2c81-4e54-e8fd-fa85f130c979"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Iâ€™m not able to see your name. If youâ€™d like me to address you personally, just let me know what youâ€™d prefer to be called!', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" The assistant doesn\\'t have any prior info about the user. According to policy, we must respond that we don\\'t know. We can ask politely for clarification.'}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 75, 'total_tokens': 154, 'completion_time': 0.156947468, 'prompt_time': 0.003092402, 'queue_time': 0.212843506, 'total_time': 0.16003987}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8db49de948', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--5f2c8406-fef1-4eaf-be11-272d05c5fc10-0', usage_metadata={'input_tokens': 75, 'output_tokens': 79, 'total_tokens': 154})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model.invoke([HumanMessage(content=\"What's my name?\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8835fa42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8835fa42",
        "outputId": "334d836d-6905-4a1d-dfc2-993d51cff2f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Yash. ðŸ˜Š', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" We have context: earlier they said \"Hi! I am Yash\". So name is Yash. Should answer accordingly.'}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 104, 'total_tokens': 154, 'completion_time': 0.10686029, 'prompt_time': 0.004861757, 'queue_time': 0.407175644, 'total_time': 0.111722047}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_82669fd91d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--82895186-caa3-402e-9e01-c7b2afabfc45-0', usage_metadata={'input_tokens': 104, 'output_tokens': 50, 'total_tokens': 154})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"Hi! I am Yash\"),\n",
        "        AIMessage(content='Hello Yash! ðŸ‘‹ How can I help you today?'),\n",
        "        HumanMessage(content=\"What's my name?\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Message Persistence"
      ],
      "metadata": {
        "id": "R0CwXaZ2GA0O"
      },
      "id": "R0CwXaZ2GA0O"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6d4a4875",
      "metadata": {
        "id": "6d4a4875"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "  response = model.invoke(state[\"messages\"])\n",
        "  return {\"messages\": response}\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "DNxqxM3eGOld"
      },
      "id": "DNxqxM3eGOld",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
      ],
      "metadata": {
        "id": "kViOe8l0HGC-"
      },
      "id": "kViOe8l0HGC-",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Hi! I'm Yash.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l5FGPpWHjAb",
        "outputId": "2fc72cf3-239b-465f-af84-1650800695ae"
      },
      "id": "5l5FGPpWHjAb",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='2ef7dfe9-8a1c-4f77-86f6-ba1dc62078b6'),\n",
              "  AIMessage(content='I donâ€™t have any information about your name. If youâ€™d like, feel free to tell me what youâ€™d like to be called, and Iâ€™ll use that!', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" There\\'s no prior context. According to policy, we must not fabricate personal info. We should respond that we don\\'t have that info. Apologize and ask if they\\'d like to share.'}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 75, 'total_tokens': 164, 'completion_time': 0.188806465, 'prompt_time': 0.003646782, 'queue_time': 0.53532412, 'total_time': 0.192453247}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_8db49de948', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--27406027-b41e-482e-abd2-0e4015066401-0', usage_metadata={'input_tokens': 75, 'output_tokens': 89, 'total_tokens': 164}),\n",
              "  HumanMessage(content=\"Hi! I'm Yash.\", additional_kwargs={}, response_metadata={}, id='6dfbdf3c-e628-48b4-a976-b1d6c1935be2'),\n",
              "  AIMessage(content='Nice to meet you, Yash! How can I help you today?', additional_kwargs={'reasoning_content': \"The user says Hi! I'm Yash. Should respond friendly, address them as Yash. No disallowed content.\"}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 125, 'total_tokens': 174, 'completion_time': 0.104868901, 'prompt_time': 0.005078415, 'queue_time': 0.27024528, 'total_time': 0.109947316}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_213abb2467', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--46445912-9b9d-4231-a0b8-34320791d2d0-0', usage_metadata={'input_tokens': 125, 'output_tokens': 49, 'total_tokens': 174})]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJT2V-wnHs_R",
        "outputId": "6c370043-de1d-429a-d569-69747e3e1f41"
      },
      "id": "dJT2V-wnHs_R",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Nice to meet you, Yash! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdidJYldIN1z",
        "outputId": "c7858db4-30c8-4ffa-b66c-da8bfa78089b"
      },
      "id": "KdidJYldIN1z",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Yash.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we change the config i.e., thread_id. It will start a new conversation\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWd1RwHRITfR",
        "outputId": "ca2103aa-f991-4ea7-ffc8-c845065d44f8"
      },
      "id": "lWd1RwHRITfR",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I donâ€™t have any information about your name. If youâ€™d like to share it, feel free to let me know!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wWlNTXCIf5T",
        "outputId": "f9a5566b-674e-4c1f-a2f2-23bac990d80a"
      },
      "id": "9wWlNTXCIf5T",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Yash.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2allI_lKIjyr"
      },
      "id": "2allI_lKIjyr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}